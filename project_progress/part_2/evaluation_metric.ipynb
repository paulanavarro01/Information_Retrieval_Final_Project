{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e5bfb3",
   "metadata": {},
   "source": [
    "EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9271529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # one level up from part_2\n",
    "sys.path.append(project_root)\n",
    "from part_2.indexing_evaluation import load_processed_docs,create_index_tfidf,search_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120aa3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Precision@K measures how many of the top K predicted items \n",
    "    are actually relevant (i.e., positive in y_true).\n",
    "    \n",
    "    Args:\n",
    "        y_true = real binary labels (1 for relevant and 0 for not relevant)\n",
    "        y_score = predicted labels\n",
    "        k = number of top-scored items to consider (by default set to 10)\n",
    "    \n",
    "    Returns:\n",
    "        Precision at rank k (between 0 and 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    order=np.argsort(np.asarray(y_score))[::-1]\n",
    "    y_true= np.take(np.asarray(y_true),order)\n",
    "    k=min(k,len(y_true)) #handle if k> len(y_true)\n",
    "    return np.sum(y_true[:k])/k if k>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55678d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Recall@K measures how many of the *relevant* items are found \n",
    "    in the top K predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        recall at rank k\n",
    "    \"\"\"\n",
    "    order=np.argsort(np.asarray(y_score))[::-1]\n",
    "    y_true=np.take(np.asarray(y_true),order)\n",
    "    relevenat=np.sum(y_true[:k])\n",
    "    total_rel=np.sum(np.asarray(y_true))\n",
    "    return relevenat/total_rel if total_rel>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13938fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Compute F1@K (F1-score at rank K)\n",
    "    It provides a balanced measure of how well the model retrieves \n",
    "    relevant items among the top K predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        F1-score at rank K (between 0 and 1)\n",
    "    \"\"\"\n",
    "    prec=precision_k(y_true,y_score,k)\n",
    "    rec=recall_k(y_true,y_score,k)\n",
    "    return (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d47ae626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Compute Average Precision@K (AP@K)\n",
    "    It averages the precision values obtained every time a relevant item (y_true == 1) is found among the top K results.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        Average precision at rank k (between 0 and 1)\n",
    "    \"\"\"\n",
    "    order=np.argsort(y_score)[::-1]\n",
    "    y_true=np.take(np.asarray(y_true),order)\n",
    "    prec_list=[]\n",
    "    num_relevant=0\n",
    "    for i in range(min(k,len(order))):\n",
    "        if y_true[i]==1:\n",
    "            num_relevant +=1\n",
    "            prec_list.append(num_relevant/(i+1))\n",
    "    return np.sum(prec_list)/num_relevant if num_relevant>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78a85f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_k(dt,k):\n",
    "    \"\"\"\n",
    "    Compute Mean Average Precision@K (MAP@K)\n",
    "\n",
    "    Args:\n",
    "        dt : a DataFrame containing columns \"query_id\", \"labels\" and \"predicted_relevance\"\n",
    "        k : number of top ranked documents to consider for the quey\n",
    "\n",
    "    Returns:\n",
    "       (The overall Mean Average Precision@K across all queries, The individual Average Precision@K scores for each query)\n",
    "       \n",
    "    \"\"\"\n",
    "    avp = []\n",
    "    for q in dt[\"query_id\"].unique():  \n",
    "        curr_data = dt[dt[\"query_id\"] == q] \n",
    "        y_true=curr_data[\"labels\"].values\n",
    "        y_score=curr_data[\"predicted_relevance\"].values\n",
    "        avp.append(average_precision(y_true,y_score,k)) \n",
    "    return np.sum(avp) / len(avp), avp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d3e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Compute Reciprocal Rank@K (RR@K)\n",
    "\n",
    "    reciprocal Rank@K measures how far down the ranking \n",
    "    the *first relevant* item appears within the top K results.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "        \n",
    "    Returns:\n",
    "        Reciprocal Rank at rank k\n",
    "    \"\"\"\n",
    "    order=np.argsort(np.asarray(y_score))[::-1]\n",
    "    y_true= np.take(np.asarray(y_true),order)[:k]\n",
    "    if np.sum(y_true)==0: \n",
    "        return 0\n",
    "    return 1/(np.argmax(y_true)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb55b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Computes Discounted Cumulative Gain at rank K DCG@K\n",
    "    DCG@K measures the ranking quality by assigning higher scores \n",
    "    to relevant items that appear earlier in the ranked list.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        Discounted Cumulative Gain at rank k\n",
    "    \"\"\"\n",
    "    order=np.argsort(y_score)[::-1]\n",
    "    y_true=np.take(y_true,order[:k])\n",
    "    gain= 2**y_true-1\n",
    "    discounts=np.log2(np.arange(len(y_true))+2) #+2 is added because log2(1) is 0\n",
    "    return np.sum(gain/discounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ba42de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Computes Normalized Discounted Cumulative Gain at k NDCG@K\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        Normalized Discounted Cumulative Gain at rank k\n",
    "    \"\"\"\n",
    "    dcg= dcg_k(y_true,y_score,k)\n",
    "    idcg=dcg_k(y_true,y_true,k)\n",
    "    if not idcg:\n",
    "        return 0\n",
    "    return round(dcg/idcg,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b8b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id  Precision@5  Recalln@5  AP@5   F1@5  MRR   NDCG\n",
      "0         1          1.0      0.385  1.00  0.556  1.0  1.000\n",
      "1         2          0.8      0.400  0.95  0.533  1.0  0.854\n",
      "MAP: 0.975\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # get the current working directory\n",
    "    base_dir=os.getcwd()\n",
    "    \n",
    "    #define path files for the processed documents and the validation labels\n",
    "    proc_doc_path = os.path.join(base_dir,'..', '..', 'data', 'processed_docs.jsonl')\n",
    "    validation_path = os.path.join(base_dir,'..', '..', 'data', 'validation_labels.csv')\n",
    "    \n",
    "    #load processed documents from the path\n",
    "    docs=load_processed_docs(proc_doc_path)\n",
    "    \n",
    "    \n",
    "    # create TF-IDF index components\n",
    "    index,tf,df_counts,idf,title_index=create_index_tfidf(docs)\n",
    "    \n",
    "    # load validation data\n",
    "    validation_df=pd.read_csv(validation_path)\n",
    "\n",
    "    # queries that we will be using\n",
    "    queries={1:\"women full sleeve sweatshirt cotton\",\n",
    "             2:\"men slim jeans blue\"}\n",
    "    \n",
    "    result=[]\n",
    "\n",
    "    # new column to store the predicted relevance\n",
    "    validation_df[\"predicted_relevance\"]=0\n",
    "    \n",
    "    #loop through each query to perform retrieving and scoring\n",
    "    for q, query in queries.items():\n",
    "        retrieve_pid=search_tf_idf(query,index,tf,idf,title_index)\n",
    "        rank_score = {pid: len(retrieve_pid) - i for i, pid in enumerate(retrieve_pid)} #assigna  descending ranking score\n",
    "        \n",
    "        #update predicted relevance in the validation set\n",
    "        validation_df.loc[validation_df[\"query_id\"] == q, \"predicted_relevance\"] = validation_df.loc[validation_df[\"query_id\"] == q, \"pid\"].apply(lambda pid: rank_score.get(pid, 0))\n",
    "\n",
    "\n",
    "    #evaluate ranking metric for each quey\n",
    "    for i in queries.keys():\n",
    "        query_doc=validation_df[validation_df[\"query_id\"]==i]\n",
    "        y_true=query_doc[\"labels\"].values\n",
    "        y_score=query_doc[\"predicted_relevance\"].values\n",
    "\n",
    "        # compute the different statistics for this query\n",
    "        result.append({\n",
    "            \"query_id\":i,\n",
    "            \"Precision@5\": round(precision_k(y_true,y_score,5),3),\n",
    "            \"Recalln@5\": round(recall_k(y_true,y_score,5),3),\n",
    "            \"AP@5\": round(average_precision(y_true,y_score,5),3),\n",
    "            \"F1@5\": round(f1_k(y_true,y_score,5),3),\n",
    "            \"MRR\": round(rr_k(y_true,y_score,5),3),\n",
    "            \"NDCG\": round(ndcg_k(y_true,y_score,5),3),\n",
    "        })\n",
    "    \n",
    "    # create a DataFrame with all the results and print them\n",
    "    results_df=pd.DataFrame(result)\n",
    "    print(results_df)\n",
    "\n",
    "    # Compute Mean Average Precission accross all queries at k = 5\n",
    "    mean_avp,avp_list=map_k(validation_df,k=5)\n",
    "    print(\"MAP:\",round(mean_avp,3))  \n",
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
