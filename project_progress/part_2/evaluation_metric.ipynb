{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05e5bfb3",
   "metadata": {},
   "source": [
    "EVALUATION METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9271529d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from array import array\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"..\"))  # one level up from part_2\n",
    "sys.path.append(project_root)\n",
    "from part_2.indexing_evaluation import load_processed_docs,create_index_tfidf,search_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "120aa3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Precision@K measures how many of the top K predicted items \n",
    "    are actually relevant (i.e., positive in y_true).\n",
    "    \n",
    "    Args:\n",
    "        y_true = real binary labels (1 for relevant and 0 for not relevant)\n",
    "        y_score = predicted labels\n",
    "        k = number of top-scored items to consider (by default set to 10)\n",
    "    \n",
    "    Returns:\n",
    "        Precision at rank k (between 0 and 1)\n",
    "    \n",
    "    \"\"\"\n",
    "    order=np.argsort(np.asarray(y_score))[::-1]\n",
    "    y_true= np.take(np.asarray(y_true),order)\n",
    "    k=min(k,len(y_true)) #handle if k> len(y_true)\n",
    "    return np.sum(y_true[:k])/k if k>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55678d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Recall@K measures how many of the *relevant* items are found \n",
    "    in the top K predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        recall at rank k\n",
    "    \"\"\"\n",
    "    order=np.argsort(np.asarray(y_score))[::-1]\n",
    "    y_true=np.take(np.asarray(y_true),order)\n",
    "    relevenat=np.sum(y_true[:k])\n",
    "    total_rel=np.sum(np.asarray(y_true))\n",
    "    return relevenat/total_rel if total_rel>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13938fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Compute F1@K (F1-score at rank K)\n",
    "    It provides a balanced measure of how well the model retrieves \n",
    "    relevant items among the top K predictions.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        F1-score at rank K (between 0 and 1)\n",
    "    \"\"\"\n",
    "    prec=precision_k(y_true,y_score,k)\n",
    "    rec=recall_k(y_true,y_score,k)\n",
    "    return (2*prec*rec)/(prec+rec) if (prec+rec)>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d47ae626",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_precision(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Compute Average Precision@K (AP@K)\n",
    "    It averages the precision values obtained every time a relevant item (y_true == 1) is found among the top K results.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        Average precision at rank k (between 0 and 1)\n",
    "    \"\"\"\n",
    "    order=np.argsort(y_score)[::-1]\n",
    "    y_true=np.take(np.asarray(y_true),order)\n",
    "    prec_list=[]\n",
    "    num_relevant=0\n",
    "    for i in range(min(k,len(order))):\n",
    "        if y_true[i]==1:\n",
    "            num_relevant +=1\n",
    "            prec_list.append(num_relevant/(i+1))\n",
    "    return np.sum(prec_list)/num_relevant if num_relevant>0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "78a85f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_k(dt,k):\n",
    "    \"\"\"\n",
    "    Compute Mean Average Precision@K (MAP@K)\n",
    "\n",
    "    Args:\n",
    "        dt : a DataFrame containing columns \"query_id\", \"labels\" and \"predicted_relevance\"\n",
    "        k : number of top ranked documents to consider for the quey\n",
    "\n",
    "    Returns:\n",
    "       (The overall Mean Average Precision@K across all queries, The individual Average Precision@K scores for each query)\n",
    "       \n",
    "    \"\"\"\n",
    "    avp = []\n",
    "    for q in dt[\"query_id\"].unique():  \n",
    "        curr_data = dt[dt[\"query_id\"] == q] \n",
    "        y_true=curr_data[\"labels\"].values\n",
    "        y_score=curr_data[\"predicted_relevance\"].values\n",
    "        avp.append(average_precision(y_true,y_score,k)) \n",
    "    return np.sum(avp) / len(avp), avp  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64d3e2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rr_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Compute Reciprocal Rank@K (RR@K)\n",
    "\n",
    "    reciprocal Rank@K measures how far down the ranking \n",
    "    the *first relevant* item appears within the top K results.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "        \n",
    "    Returns:\n",
    "        Reciprocal Rank at rank k\n",
    "    \"\"\"\n",
    "    order=np.argsort(np.asarray(y_score))[::-1]\n",
    "    y_true= np.take(np.asarray(y_true),order)[:k]\n",
    "    if np.sum(y_true)==0: \n",
    "        return 0\n",
    "    return 1/(np.argmax(y_true)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb55b514",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Computes Discounted Cumulative Gain at rank K DCG@K\n",
    "    DCG@K measures the ranking quality by assigning higher scores \n",
    "    to relevant items that appear earlier in the ranked list.\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        Discounted Cumulative Gain at rank k\n",
    "    \"\"\"\n",
    "    order=np.argsort(y_score)[::-1]\n",
    "    y_true=np.take(y_true,order[:k])\n",
    "    gain= 2**y_true-1\n",
    "    discounts=np.log2(np.arange(len(y_true))+2) #+2 is added because log2(1) is 0\n",
    "    return np.sum(gain/discounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ba42de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_k(y_true,y_score,k=10):\n",
    "    \"\"\"\n",
    "    Computes Normalized Discounted Cumulative Gain at k NDCG@K\n",
    "\n",
    "    Args:\n",
    "        y_true: real binary levels (1 for relevant and 0 for not relevant)\n",
    "        y_score: predicted labels\n",
    "        k: number of top-scored items to consider (by default set to 10)\n",
    "\n",
    "    Returns:\n",
    "        Normalized Discounted Cumulative Gain at rank k\n",
    "    \"\"\"\n",
    "    dcg= dcg_k(y_true,y_score,k)\n",
    "    idcg=dcg_k(y_true,y_true,k)\n",
    "    if not idcg:\n",
    "        return 0\n",
    "    return round(dcg/idcg,4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "947b8b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   query_id  Precision@5  Recall5  AP@5   F1@5  MRR   NDCG\n",
      "0         1          1.0    0.385  1.00  0.556  1.0  1.000\n",
      "1         2          0.8    0.400  0.95  0.533  1.0  0.854\n",
      "MAP: 0.975\n"
     ]
    }
   ],
   "source": [
    "# Main code\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    \n",
    "    # get the current working directory\n",
    "    base_dir=os.getcwd()\n",
    "    \n",
    "    #define path files for the processed documents and the validation labels\n",
    "    proc_doc_path = os.path.join(base_dir,'..', '..', 'data', 'processed_docs.jsonl')\n",
    "    validation_path = os.path.join(base_dir,'..', '..', 'data', 'validation_labels.csv')\n",
    "    \n",
    "    #load processed documents from the path\n",
    "    docs=load_processed_docs(proc_doc_path)\n",
    "    \n",
    "    \n",
    "    # create TF-IDF index components\n",
    "    index,tf,df_counts,idf,title_index=create_index_tfidf(docs)\n",
    "    \n",
    "    # load validation data\n",
    "    validation_df=pd.read_csv(validation_path)\n",
    "\n",
    "    # queries that we will be using\n",
    "    queries={1:\"women full sleeve sweatshirt cotton\",\n",
    "             2:\"men slim jeans blue\"}\n",
    "    \n",
    "    result=[]\n",
    "\n",
    "    # new column to store the predicted relevance\n",
    "    validation_df[\"predicted_relevance\"]=0\n",
    "    \n",
    "    #loop through each query to perform retrieving and scoring\n",
    "    for q, query in queries.items():\n",
    "        retrieve_pid=search_tf_idf(query,index,tf,idf,title_index)\n",
    "        rank_score = {pid: len(retrieve_pid) - i for i, pid in enumerate(retrieve_pid)} #assigna  descending ranking score\n",
    "        \n",
    "        #update predicted relevance in the validation set\n",
    "        validation_df.loc[validation_df[\"query_id\"] == q, \"predicted_relevance\"] = validation_df.loc[validation_df[\"query_id\"] == q, \"pid\"].apply(lambda pid: rank_score.get(pid, 0))\n",
    "\n",
    "\n",
    "    #evaluate ranking metric for each quey\n",
    "    for i in queries.keys():\n",
    "        query_doc=validation_df[validation_df[\"query_id\"]==i]\n",
    "        y_true=query_doc[\"labels\"].values\n",
    "        y_score=query_doc[\"predicted_relevance\"].values\n",
    "\n",
    "        # compute the different statistics for this query\n",
    "        result.append({\n",
    "            \"query_id\":i,\n",
    "            \"Precision@5\": round(precision_k(y_true,y_score,5),3),\n",
    "            \"Recall5\": round(recall_k(y_true,y_score,5),3),\n",
    "            \"AP@5\": round(average_precision(y_true,y_score,5),3),\n",
    "            \"F1@5\": round(f1_k(y_true,y_score,5),3),\n",
    "            \"MRR\": round(rr_k(y_true,y_score,5),3),\n",
    "            \"NDCG\": round(ndcg_k(y_true,y_score,5),3),\n",
    "        })\n",
    "    \n",
    "    # create a DataFrame with all the results and print them\n",
    "    results_df=pd.DataFrame(result)\n",
    "    print(results_df)\n",
    "\n",
    "    # Compute Mean Average Precission accross all queries at k = 5\n",
    "    mean_avp,avp_list=map_k(validation_df,k=5)\n",
    "    print(\"MAP:\",round(mean_avp,3))  \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f1cca5c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 query  Precision@5  Recall@5  AP@5   F1@5  \\\n",
      "0              full sleeve black shirt          0.4     0.333  0.75  0.364   \n",
      "1               solid women white polo          1.0     1.000  1.00  1.000   \n",
      "2  print of multicolor neck grey shirt          1.0     0.625  1.00  0.769   \n",
      "3              slim fit men blue jeans          1.0     0.714  1.00  0.833   \n",
      "4    round collar full sleeves t-shirt          1.0     0.500  1.00  0.667   \n",
      "\n",
      "   MRR   NDCG  \n",
      "0  1.0  0.485  \n",
      "1  1.0  1.000  \n",
      "2  1.0  1.000  \n",
      "3  1.0  1.000  \n",
      "4  1.0  1.000  \n"
     ]
    }
   ],
   "source": [
    "# relevant documents for each query\n",
    "relevant_docs = {\n",
    "    \"full sleeve black shirt\": {\n",
    "        \"TSHFUTG2TZSYDWTP\", \"TSHF94NKFVSHGVWQ\", \"TSHFHFTQSYKZKFBU\", \n",
    "        \"TSHFJVVEDZKWSEHG\", \"TSHEM8ZK5BNQMX6V\", \"TSHFVX7ZFNFEMHUG\"\n",
    "    },\n",
    "    \"solid women white polo\": {\n",
    "        \"TSHFVHYZYUG6FXYB\", \"TSHFZ3JE9KCHWVYW\", \"TSHFXVJZYXJNUZH6\", \n",
    "        \"TSHFZ67FC4P49NFW\", \"TSHFUGZTQVRAWTYG\"\n",
    "    },\n",
    "    \"print of multicolor neck grey shirt\": {\n",
    "        \"TSHFU63ASBANXZVF\", \"TSHFMFXGFJ7G2ABK\", \"TSHFGH2XTNZFHENV\", \"TSHFGH2XNZZUGZVA\",\n",
    "        \"TSHFGH2XHSKYJBJY\", \"TSHFGH2XZRZHEK8Y\", \"TSHFGH2YPRKAHHN2\", \"TSHFGH2YMYXKKSTB\"\n",
    "    },\n",
    "    \"slim fit men blue jeans\": {\n",
    "        \"JEAFDZRVABZZ2PQ3\", \"JEAFDHHGY3HBXEMP\", \"JEAFDWG9K7KPKHS8\", \n",
    "        \"JEAFEC2GEMGBWHA5\", \"JEAFDWC8NGRWDHRS\", \"JEAFJM2P2FQQJF3C\", \"JEAFDWCPDCVQAA3T\"\n",
    "    },\n",
    "    \"round collar full sleeves t-shirt\": {\n",
    "        \"SHTFZHY7FJ5Z2V5Q\", \"SHTFZHY7XZZDGYPN\", \"SHTFZHY7J7R56N3C\", \"SHTFSYHDXAGGA2WR\", \n",
    "        \"SHTFSYHDK3H6HN7H\", \"SHTFMYXDPCMF7ZWG\", \"SHTFMYXDAFGZMNFC\", \"SHTFMYXDZSNGGSCY\", \n",
    "        \"SHTFMYXDRXZPVXJA\", \"SHTFZKXC2QSQMGHG\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# queries in part 1\n",
    "queries = [\n",
    "    \"full sleeve black shirt\",\n",
    "    \"solid women white polo\",\n",
    "    \"print of multicolor neck grey shirt\",\n",
    "    \"slim fit men blue jeans\",\n",
    "    \"round collar full sleeves t-shirt\"\n",
    "]\n",
    "\n",
    "# store binary labels per query\n",
    "binary_labels_per_query = {}\n",
    "\n",
    "# assign binary labels\n",
    "for query in queries:\n",
    "    retrieved_docs = search_tf_idf(query, index, tf, idf, title_index)\n",
    "    binary_labels = [1 if doc_id in relevant_docs[query] else 0 for doc_id in retrieved_docs]\n",
    "    binary_labels_per_query[query] = binary_labels\n",
    "\n",
    "\n",
    "# calculate evaluation metrics and collect results\n",
    "results = []\n",
    "for query in queries:\n",
    "    retrieved_docs = search_tf_idf(query, index, tf, idf, title_index)\n",
    "    relevant_set = relevant_docs[query]\n",
    "\n",
    "    y_true = [1 if doc_id in relevant_set else 0 for doc_id in retrieved_docs]\n",
    "\n",
    "    rank_score = {doc_id: len(retrieved_docs) - i for i, doc_id in enumerate(retrieved_docs)}\n",
    "    y_score = [rank_score.get(doc_id, 0) for doc_id in retrieved_docs]\n",
    "\n",
    "    results.append({\n",
    "        \"query\": query,\n",
    "        \"Precision@5\": round(precision_k(y_true, y_score, 5), 3),\n",
    "        \"Recall@5\": round(recall_k(y_true, y_score, 5), 3),\n",
    "        \"AP@5\": round(average_precision(y_true, y_score, 5), 3),\n",
    "        \"F1@5\": round(f1_k(y_true, y_score, 5), 3),\n",
    "        \"MRR\": round(rr_k(y_true, y_score, 5), 3),\n",
    "        \"NDCG\": round(ndcg_k(y_true, y_score, 5), 3)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "irwa_venv (3.13.9)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
